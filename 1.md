# NLP - Sequatial에서 BERT까지

### Encoder - Decoder Structure

NLP 분야에서 딥 러닝이 활용되는 거의 모든 Task에는 문장을 이해하거나 생성하는 과정이 담겨 있다. 자연어 단어나 문장은 일정한 구조를 가진 시퀀스로 해석할 수 있다. 영어를 예로 들어 보자.

- 하나의 단어는 알파벳들이 특정한 규칙에 따라 모여 의미를 형성한다. 단어 'model'은 m, o, d, e, l의 알파벳으로 각각 분할 가능하다.
- 하나의 문장은 단어와 문장부호들이 구조적으로 나열되어 의미를 형성한다. 문장 'I see lovely cat.'은 'I', 'see', 'lovely', 'cat', '.'(온점)의 각 단어로 분할하여 해석할 수 있다.

따라서 딥 러닝 모델이 자연어 입력을 이해하려면, 먼저 입력 데이터의 형태에 맞춰 **구조적으로 분할**하여 해석해야 한다. 즉, 특수한 형태의 시퀀스 데이터로써 바라보아야 한다. 또한 그렇게 해석한 내용을 바탕으로 결과 데이터를 **구조적으로 맞추어** 생성해야 한다. 번역 모델이라면 번역 결과 문장일 것이고, 질답 모델이라면 입력 질문에 대한 답변 문장일 것이다.

이 문제를 풀기 위해 **인코더-디코더** 구조가 제안되었다. 인코더 모델과 디코더 모델을 합친 구조이고, 둘 다 RNN으로 구성된다.

**인코더**는 입력 데이터를 분할하여 전체적인 의미를 해석한다. 자연어 입력 데이터는 먼저 **토큰** 단위로 분할된 후 인코더에 입력된다. 토큰은 데이터를 쪼갰을 때 의미를 가질 수 있는 최소 단위로 이해하면 되고, 문장 데이터에서는 각각의 단어가 하나의 토큰이 된다. 인코더 RNN은 문장이 끝날 때까지 다음 토큰을 받으면서 셀 연산을 반복한다. 모든 토큰을 해석하고 난 후 최종적으로 남은 RNN 셀의 Hidden State에는 입력 문장을 해석한 정보가 담겨 있게 된다. 이 Hidden State를 **컨텍스트 벡터(Context Vector)** 라고 부른다.

<center><img src="https://user-images.githubusercontent.com/44392433/124483317-27ce1a80-dde5-11eb-842a-b6f4e3bfcee7.PNG" width=400></center>

인코더가 해석한 문장 내용 데이터를 바탕으로 출력을 생성하는 과정은 **디코더**가 담당한다. 디코더는 컨텍스트 벡터를 입력으로 받아 다시 자연어 문장을 출력하는 RNN 모델이다. 자연어 문장의 생성은 앞에 위치한 단어를 바탕으로 뒤에 따라붙을 단어를 예측하는 방식으로 구현된다. 먼저 문장의 시작을 알리는 특수한 토큰을 RNN 셀에 처음으로 입력하면, 셀은 출력 문장에서 첫 번째로 위치할 단어를 유추하여 출력한다. 셀의 출력은 다시 셀으로 입력되어 다음으로 위치할 단어의 출력을 반복한다. 그리고 이 과정에서 디코더 RNN 셀의 Hidden State에 컨텍스트 벡터로부터 전달받은 입력 문장 정보를 계속 유지한다. 이 과정을 통해 입력 데이터와 연관된 출력을 생성할 수 있도록 만드는 것이 디코더의 핵심 아이디어다.

<center><img src="https://user-images.githubusercontent.com/44392433/124483346-30beec00-dde5-11eb-9bbe-798ba38538bb.PNG" width=450></center>

인코더-디코더 구조는 Sequance-to-Sequance(**Seq2Seq**)라고도 불리며, 대부분의 NLP 분야에서 딥 러닝을 활용할 때 기초적인 구조로 널리 활용되었다.

### Attention

인코더-디코더와 같이 RNN을 기반으로 한 전통적인 자연어 해석 모델은 분명 NLP 분야의 획기적인 발전을 가져왔지만, 몇 가지 명확한 한계를 지니고 있었다.

- 입력 데이터가 아무리 길어져도 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축해야 한다. 이는 정보의 손실을 일으킨다.
- RNN 구조의 고질적인 문제인 그래디언트 소실 현상이 발생하여 문장의 앞 부분에 대한 정보를 잃어버린다. 입력 데이터가 길수록 이 문제는 크게 다가온다.
- 각각의 RNN 셀이 입력 데이터를 해석할 때, 모든 토큰의 중요도를 동일하게 이해한다. 문장을 이해할 때 핵심 키워드에 집중하는 인간의 직관과 거리가 있다.

세 가지 문제 모두 RNN 기반 모델의 구조적 한계에서 발생하고 있다. 입력 데이터가 길어질수록 출력 데이터의 품질은 떨어질 수밖에 없으므로, 성능 향상의 한계에 봉착하게 된다. 그리고 이를 해결하기 위해 모델 구조에 **어텐션(Attention)** 을 적용한 기법이 2017년에 발표되면서 NLP는 새로운 국면을 맞이하게 된다.
