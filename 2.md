### Attention

인코더-디코더와 같이 RNN을 기반으로 한 전통적인 자연어 해석 모델은 분명 NLP 분야의 획기적인 발전을 가져왔지만, 몇 가지 명확한 한계를 지니고 있었다.

- 입력 데이터가 아무리 길어져도 모든 정보를 고정된 크기의 컨텍스트 벡터에 압축해야 한다. 이는 정보의 손실을 일으킨다.
- RNN 구조의 고질적인 문제인 그래디언트 소실 현상이 발생하여 문장의 앞 부분에 대한 정보를 잃어버린다. 입력 데이터가 길수록 이 문제는 크게 다가온다.
- 각각의 RNN 셀이 입력 데이터를 해석할 때, 모든 토큰의 중요도를 동일하게 이해한다. 문장을 이해할 때 핵심 키워드에 집중하는 인간의 직관과 거리가 있다.

세 가지 문제 모두 RNN 기반 모델의 구조적 한계에서 발생하고 있다. 입력 데이터가 길어질수록 출력 데이터의 품질은 떨어질 수밖에 없으므로, 성능 향상의 한계에 봉착하게 된다. 그리고 이를 해결하기 위해 모델 구조에 **어텐션(Attention)**을 적용한 기법이 2017년에 발표되면서 NLP는 새로운 국면을 맞이하게 된다.

#### What is Attention?

어텐션은 딥 러닝 모델이 데이터의 일부에 더욱 집중하여 분석하도록 돕는 기법이다. 대부분의 경우 특정한 계산법에 따라 데이터들의 중요도를 수치화하고, 이를 **가중치(Weight)** 형태로 모델에 적용하는 방법을 사용한다.

어텐션의 개념은 매우 단순하다. 심지어 우리는 지금까지 모든 모델의 학습에 일종의 어텐션을 사용하고 있었다. 단순히 입력 데이터 일부에 가중치를 주어 모델 출력을 변경하는 예측 모델도 어텐션을 이용한다고 할 수 있기 때문이다. 즉, 각각의 입력 데이터에 서로 다른 가중치를 곱해 출력 데이터를 만드는 단순한 모델도 가중치라는 이름의 어텐션 메커니즘을 사용하고 있던 것이다.

이러한 메커니즘이 **어텐션**으로 불리기 시작한 역사는 1960년대로 거슬러 올라간다. 그 당시 발표된 논문에서 입력 데이터마다 다른 가중치를 두어 출력 결과를 만드는 모델을 설계하고, 각각의 가중치를 각 입력에 대한 어텐션의 정도로 정의하였다. 해당 논문에서는 모델을 다음과 같이 정의한다.

- 모델의 입력 데이터 ![](https://latex.codecogs.com/png.latex?%5Cinline%20X%20%3D%20%7Bx_1%2C%20x_2%2C%20...%2C%20x_n%7D)와 정답 데이터 ![](https://latex.codecogs.com/png.latex?%5Cinline%20Y%20%3D%20%7By_1%2C%20y_2%2C%20...%2C%20y_n%7D)를 쌍으로 가지는 학습 데이터셋 ![](https://latex.codecogs.com/png.latex?%5Cinline%20%7B%28x_1%2C%20y_1%29%2C%20%28x_2%2C%20y_2%29%2C%20...%2C%20%28x_n%2C%20y_n%29%7D)이 있다고 가정한다.
- 일단 모델을 어떤 입력이 들어와도 모든 정답 데이터의 평균을 출력하도록 다음과 같이 설정한다 : ![](https://latex.codecogs.com/png.latex?%5Cinline%20%5Chat%7By%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum%5E%7Bn%7D_%7Bi%7Dy_i)
- 이제 모든 입력 데이터에 대해, 해당 입력 데이터가 모델 출력에 미치는 상대적인 영향을 계산하는 어텐션 함수를 ![](https://latex.codecogs.com/png.latex?%5Cinline%20%5Calpha%28x%2C%20x_i%29)로 정의한다. 어텐션 함수는 입력 데이터 ![](https://latex.codecogs.com/png.latex?%5Cinline%20x)에 대해 모델이 결과를 출력할 때, ![](https://latex.codecogs.com/png.latex?%5Cinline%20i)번째 데이터인 ![](https://latex.codecogs.com/png.latex?%5Cinline%20%28x_i%2C%20y_i%29)가 상대적으로 얼마나 더 중요한지를 수치화하는 역할이다.
- 모델이 어텐션을 이용하도록 다음과 같이 수정한다 : ![](https://latex.codecogs.com/png.latex?%5Cinline%20%5Chat%7By%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum%5E%7Bn%7D_%7Bi%7D%5Calpha%28x%2C%20x_i%29y_i)
- 이제 모델은 입력 데이터에 대해 상대적으로 더 중요한 데이터를 결과에 더 많이 반영하고, 덜 중요한 데이터는 적게 반영한다. 즉, 각각의 입력 데이터들을 바라보는 모델의 집중도를 다르게 해서 입력에 따른 알맞은 출력을 만들도록 한다.

시간이 지나 딥 러닝이 발전하면서, '입력 데이터마다 다른 가중치를 두어 출력 데이터를 개선한다'는 아이디어는 딥 러닝 모델의 가중치 학습으로 일반화되었다. 그리고 어텐션이라는 아이디어는 더 진화하여, 가중치 등 기존의 파라미터들과 별개로 딥 러닝 모델이 **선택과 집중**을 할 수 있도록 하는 하나의 메커니즘의 의미를 가지게 된다.

#### Attention Model

(그림 다시 그려서 재작성)

다시 인코더-디코더 구조로 돌아오자. 인코더-디코더 구조에 어텐션을 적용함으로써 기대하는 현상은, 디코더가 출력 시퀀스를 생성할 때 각 입력 토큰들의 중요도를 다르게 인식하는 것이다. 디코더는 컨텍스트 벡터로부터 출력 시퀀스를 생성하므로, 컨텍스트 벡터에 어텐션을 적용하여 생성하는 편이 가장 확실하다.

기존의 인코더-디코더 구조에서 컨텍스트 벡터는 단순히 인코더의 최종 Hidden State였다. 그래서 벡터의 길이가 고정될 수밖에 없었고, 위에서 이야기한 문제점들이 발생하였다. 그래서 Attention Model은 디코더 RNN 셀의 각 반복마다 해당 시점에 필요한 Context를 직접 생성한다. 그러면 컨텍스트 벡터의 길이가 가변적이게 되고, 컨텍스트 벡터에서의 정보 손실 문제가 어느 정도 해결된다.

각 시점에서 컨텍스트 벡터를 계산할 때, 어텐션 값이 같이 계산되어 컨텍스트 벡터에 반영된다. 각 시점의 어텐션 값들은 다음과 같이 Matrix 형태로 나타난다.

<center><img src="https://user-images.githubusercontent.com/44392433/127002726-b83d89f0-1b25-48c9-b1ed-5bd7144cc51b.PNG" width=200></center>
<center><그림 3> Attention Matrix </center>

![](https://latex.codecogs.com/png.latex?%5Cinline%20%28h_1%2C%20h_2%2C%20...%2C%20h_n%29)은 인코더의 Hidden State, ![](https://latex.codecogs.com/png.latex?%5Cinline%20%28s_1%2C%20s_2%2C%20...%2C%20s_n%29)는 디코더의 Hidden State이다. 당연히 ![](https://latex.codecogs.com/png.latex?%5Cinline%20%28h_1%2C%20h_2%2C%20...%2C%20h_n%29)은 고정되어 있고, 디코더 셀의 ![](https://latex.codecogs.com/png.latex?%5Cinline%20n)번째 반복에서 ![](https://latex.codecogs.com/png.latex?%5Cinline%20s_n)을 바탕으로 ![](https://latex.codecogs.com/png.latex?%5Cinline%20%28%5Calpha_1%2C%20%5Calpha_2%2C%20...%2C%29)가 계산된다. 이렇게 계산된 어텐션 값들은 다음과 같이 해당 시점의 컨텍스트 벡터를 만드는 데에 사용된다.

<center><img src="https://latex.codecogs.com/png.latex?%5CLARGE%20c_t%20%3D%20%5Calpha_t_1h_1%20&plus;%20%5Calpha_t_2h_2%20&plus;%20...%20&plus;%20%5Calpha_c_nh_n" width=300></center>

이 컨텍스트 벡터는 인코더 Hidden State의 각 내용에 일정한 가중치를 준 듯한 모양을 하고 있다. 해당 시점의 디코더가 입력 토큰 중 어느 것에 더 집중해야 하는지에 대한 정보를 포함하도록 하는 것이다. 따라서 어텐션은 현재 시점 디코더의 Hidden State와 인코더의 Hidden State 사이의 모든 관계를 특정 함수로 계산하여 만들어진다. 결과적으로 어텐션은, 디코더가 해당 시점의 출력 토큰을 만들 때 어떤 입력 토큰들에 더욱 집중해야 하는지에 대한 정보를 만들어준다. 그래서 어텐션의 계산에 인코더와 디코더의 매 시점 Hidden State가 모두 필요한 것이다. 매 시점마다 디코더가 상대적으로 더욱 집중해야 하는 토큰이 달라질 수 있기 때문이다. 그리고 이렇게 해야 더욱 구조적으로 완성도 있는 출력 데이터를 만들어낼 수 있다. 이후에 실제 출력 토큰을 생성하는 방식은 기존 인코더-디코더 방식과 동일하다.
